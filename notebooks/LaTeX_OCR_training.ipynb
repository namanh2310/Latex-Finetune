{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YtR1GhYwnLnu"
      },
      "source": [
        "# Train a LaTeX OCR model\n",
        "In this brief notebook I show how you can finetune/train an OCR model.\n",
        "\n",
        "I've opted to mix in handwritten data into the regular pdf LaTeX images. For that I started out with the released pretrained model and continued training on the slightly larger corpus."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "print(torch.__version__)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(torch.cuda.get_device_name())\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r396ah-Q3EQc"
      },
      "outputs": [],
      "source": [
        "%pip install pix2tex[train] -qq\n",
        "%pip install tensorflow\n",
        "%pip install gpustat -q\n",
        "%pip install opencv-python-headless==4.1.2.30 -U -q\n",
        "%pip install --upgrade --no-cache-dir gdown -q\n",
        "%pip install gdown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!gpustat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "print(tf.config.list_physical_devices('GPU'))\n",
        "\n",
        "if tf.config.experimental.list_physical_devices('GPU'):\n",
        "    print(\"GPU is available\")\n",
        "else:\n",
        "    print(\"No GPU detected\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "os.makedirs(\"dataset/data\")\n",
        "os.makedirs(\"images\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# em xem rồi xóa đi dataset của thằng handwritten để train trên tập 100k nhe :V\n",
        "\n",
        "import os\n",
        "import subprocess\n",
        "\n",
        "os.makedirs('dataset/data', exist_ok=True)\n",
        "os.makedirs('images', exist_ok=True)\n",
        "\n",
        "def gdown(id, output):\n",
        "    subprocess.run(['gdown', '--id', id, '-O', output], check=True)\n",
        "\n",
        "handwritten_id = '13vjxGYrFCuYnwgDIUqkxsNGKk__D_sOM'\n",
        "pdf_images_id = '176PKaCUDWmTJdQwc-OfkO0y8t4gLsIvQ'\n",
        "pdf_math_id = '1QUjX6PFWPa-HBWdcY-7bA5TRVUnbyS1D'\n",
        "\n",
        "gdown(handwritten_id, 'dataset/data/crohme.zip')\n",
        "gdown(pdf_images_id, 'dataset/data/pdf.zip')\n",
        "gdown(pdf_math_id, 'dataset/data/pdfmath.txt')\n",
        "\n",
        "os.chdir('dataset/data')\n",
        "\n",
        "subprocess.run(['unzip', '-q', 'crohme.zip'], check=True)\n",
        "subprocess.run(['unzip', '-q', 'pdf.zip'], check=True)\n",
        "\n",
        "os.chdir('images')\n",
        "\n",
        "os.makedirs('../valimages', exist_ok=True)\n",
        "\n",
        "shuffled_files = subprocess.run(['ls'], capture_output=True, text=True)\n",
        "files = shuffled_files.stdout.split()\n",
        "subprocess.run(['shuf', '-n', '1000'], input=\"\\n\".join(files), text=True, capture_output=True, check=True)\n",
        "selected_files = subprocess.run(['shuf', '-n', '1000'], input=\"\\n\".join(files), text=True, capture_output=True)\n",
        "selected_files_list = selected_files.stdout.split()\n",
        "\n",
        "for file in selected_files_list:\n",
        "    os.rename(file, f'../valimages/{file}')\n",
        "\n",
        "os.chdir('../../..')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2BMuIqRIqG-8"
      },
      "source": [
        "Now we generate the datasets. We can string multiple datasets together to get one large lookup table. The only thing saved in these pkl files are image sizes, image location and the ground truth latex code. That way we can serve batches of images with the same dimensionality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1JebcEarl-g6"
      },
      "outputs": [],
      "source": [
        "!python -m pix2tex.dataset.dataset -i dataset/data/images dataset/data/train -e dataset/data/CROHME_math.txt dataset/data/pdfmath.txt -o dataset/data/train.pkl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x_Orutb37xHD"
      },
      "outputs": [],
      "source": [
        "!python -m pix2tex.dataset.dataset -i dataset/data/valimages dataset/data/val -e dataset/data/CROHME_math.txt dataset/data/pdfmath.txt -o dataset/data/val.pkl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I3iOyEEBbw58"
      },
      "outputs": [],
      "source": [
        "# download the weights we want to fine tune\n",
        "!curl -L -o weights.pth https://github.com/lukas-blecher/LaTeX-OCR/releases/download/v0.0.1/weights.pth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vow2NnpHmWt0"
      },
      "outputs": [],
      "source": [
        "# If using wandb\n",
        "%pip install -Uq wandb \n",
        "# you can cancel this if you don't wan't to use it or don't have a W&B acc.\n",
        "#!wandb login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OnsNCLp84QSY"
      },
      "outputs": [],
      "source": [
        "# generate colab specific config (set 'debug' to true if wandb is not used)\n",
        "!echo {backbone_layers: [2, 3, 7], betas: [0.9, 0.999], batchsize: 10, bos_token: 1, channels: 1, data: dataset/data/train.pkl, debug: true, decoder_args: {'attn_on_attn': true, 'cross_attend': true, 'ff_glu': true, 'rel_pos_bias': false, 'use_scalenorm': false}, dim: 256, encoder_depth: 4, eos_token: 2, epochs: 50, gamma: 0.9995, heads: 8, id: null, load_chkpt: 'weights.pth', lr: 0.001, lr_step: 30, max_height: 192, max_seq_len: 512, max_width: 672, min_height: 32, min_width: 32, name: mixed, num_layers: 4, num_tokens: 8000, optimizer: Adam, output_path: outputs, pad: false, pad_token: 0, patch_size: 16, sample_freq: 2000, save_freq: 1, scheduler: StepLR, seed: 42, temperature: 0.2, test_samples: 5, testbatchsize: 20, tokenizer: dataset/tokenizer.json, valbatches: 100, valdata: dataset/data/val.pkl} > colab.yaml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import wandb\n",
        "\n",
        "wandb.login()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c8NU5j2k3z36"
      },
      "outputs": [],
      "source": [
        "!python -m pix2tex.train --config colab.yaml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g3DU9KxubWgq"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "LaTeX-OCR training.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
